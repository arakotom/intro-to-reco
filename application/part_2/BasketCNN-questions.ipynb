{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Basket Completion\n",
    "\n",
    "The recommendation task of basket completion is a key part of many online retail applications. Basket completion involves computing predictions for the next item that should be added to a shopping basket, given a collection of items that the user has already added to the basket.\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "Amazon Baby Registries - This is a public dataset that consists of registries of baby products\n",
    "from 15 different categories (such as ’feeding’, ’diapers’,’toys’, etc.), where the item catalog and registries for each category are disjoint. Each category therefore provides a small dataset, with a maximum of 15,000 purchased baskets per category. \n",
    "\n",
    "\n",
    "\n",
    "# Solution:\n",
    "\n",
    "**DNN with 1Dimensional CNNs** - Similar to what we presented in the TextCNN section of our course.\n",
    "\n",
    "![TextCNN](./images/textCNN.png)\n",
    "\n",
    "\n",
    "\n",
    "## About the Conv2D operator in TF\n",
    "\n",
    "\n",
    "**Inputs:** \n",
    "\n",
    "* In_Tensor = [ batch_size; seq_length; embedding_size; 1]\n",
    "\n",
    "* Filter = [filter_size; embedding_size; 1; num_conv_filters]\n",
    "\n",
    "**Output:**\n",
    "* Out_Tensor = [batch_size; new_length; 1; num_conv_filters]\n",
    "\n",
    "* where: new_length = seq_length - filter_size + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Metrics:\n",
    "\n",
    "### * MPR: Mean Percentile Ranking\n",
    "### * Precision@k\n",
    "\n",
    "\n",
    "-----------------------------\n",
    "-----------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Questions:\n",
    "\n",
    "### Q1. Add a second convolutional layer in Gen_CNN_Model() and compare its performance against the initial architecture\n",
    "\n",
    "### Q2. Tune the convolution parameters and compare the results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from random import shuffle\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, Dropout, Bidirectional, Input, Masking, Conv1D, MaxPooling1D\n",
    "from keras.layers import Reshape,Conv2D, MaxPooling2D, Flatten, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras import losses\n",
    "\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "LOCAL_PATH = \"./data/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Various utils\n",
    "\n",
    "def get_basket_set_size(data, batch_size, seq_length):\n",
    "    batch = list()\n",
    "    while len(batch)<batch_size:\n",
    "        line = np.random.randint(0, len(data))\n",
    "        basket = data[line]\n",
    "        if len(basket)>=seq_length:\n",
    "            batch.append(list(np.random.choice(basket, size=seq_length, replace=False)))\n",
    "    return np.array(batch)\n",
    "\n",
    "\n",
    "def counters_per_prod(data):\n",
    "    counter = Counter()\n",
    "    for elem in data:\n",
    "        counter.update(elem)\n",
    "    list_of_keys = np.unique(np.array(list(counter.keys())))\n",
    "    print(\"Vocabulary size with unique items\", len(list_of_keys))\n",
    "    return counter, len(list_of_keys)\n",
    "\n",
    "\n",
    "def get_popularity_dist(training_data, vocabulary_size):\n",
    "    counter = Counter()\n",
    "    for elem in training_data:\n",
    "        counter.update(elem)\n",
    "    popularity_dist = [float(counter[i]) for i in range(vocabulary_size)]\n",
    "    return popularity_dist\n",
    "\n",
    "\n",
    "## data reading\n",
    "\n",
    "def read_data(data_file_path):\n",
    "    data = [[]]\n",
    "    with open(data_file_path) as data_csv_file:\n",
    "        data_csv_reader = csv.reader(data_csv_file, delimiter = ',')\n",
    "        for row in data_csv_reader:\n",
    "            if len(row) == 1:\n",
    "                continue\n",
    "            data.append([(int(i) - 1) for i in row])\n",
    "    del data[0]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "    if dataset == \"Amazon\":\n",
    "        data = read_data(LOCAL_PATH + \"1_100_100_100_apparel_regs.csv\")\n",
    "        folder = LOCAL_PATH + \"Amazon/\"\n",
    "\n",
    "    print(\"min elem = \", min([elem for ss in data for elem in ss]), \" max elem = \", max([elem for ss in data for elem in ss]))\n",
    "    return data, folder\n",
    "\n",
    "\n",
    "### batching functions\n",
    "\n",
    "def create_batch(self):\n",
    "    seq_length = np.random.randint(8)+2\n",
    "    batches = get_basket_set_size(self.training_data, self.batch_size, seq_length)\n",
    "    train_words, label_words = batches[:,:-1], batches[:,1:]\n",
    "    return train_words, label_words\n",
    "\n",
    "\n",
    "def generate_random_bundles(list_of_sizes, number_of_products):\n",
    "    products = np.arange(number_of_products)\n",
    "    sets = list()\n",
    "    for size in list_of_sizes:\n",
    "        sets.append(np.random.choice(products, size=size, replace=False))\n",
    "    return sets\n",
    "\n",
    "\n",
    "def get_test_list_batches(self, max_basket_size, number_of_baskets_per_size):\n",
    "    test_data = copy.copy(self.test_data)\n",
    "    self.test_list_batches = list()\n",
    "\n",
    "    for elem in test_data:\n",
    "        random.shuffle(elem)\n",
    "        \n",
    "    for i in range(2, max_basket_size):\n",
    "        self.test_list_batches.append(get_basket_set_size(test_data, number_of_baskets_per_size, i))\n",
    "\n",
    "    self.test_list_batches.insert(0, [])\n",
    "    self.test_list_batches.insert(0, [])\n",
    "    print(self.test_list_batches[0][:5])\n",
    "    print(self.test_list_batches[1][:5])\n",
    "    print(self.test_list_batches[2][:5])\n",
    "    print(self.test_list_batches[3][:5])\n",
    "    return self.test_list_batches\n",
    "\n",
    "\n",
    "### performance functions\n",
    "\n",
    "def print_results_predictions(last_predictions, batch_inputs, targets, vocabulary_size):\n",
    "    predictions_for_targets = 100*np.mean(np.array([last_predictions[i][int(targets[i])] for i in range(len(batch_inputs))]))\n",
    "    predictions_for_random = 100*np.mean(np.array([last_predictions[i][np.random.choice(vocabulary_size-1, 20, replace=False)] for i in range(len(batch_inputs))]))\n",
    "\n",
    "    predictions_top_one = [1 if targets[i] in list(np.argpartition(-last_predictions[i], 1)[:1]) else 0 for i in range(len(batch_inputs))]\n",
    "    predictions_top_one = 100*np.count_nonzero(np.array(predictions_top_one))/len(batch_inputs)\n",
    "    predictions_top_five = [1 if targets[i] in list(np.argpartition(-last_predictions[i], 5)[:5]) else 0 for i in range(len(batch_inputs))]\n",
    "    predictions_top_five = 100*np.count_nonzero(np.array(predictions_top_five))/len(batch_inputs)\n",
    "\n",
    "    percent = int(vocabulary_size*0.01)\n",
    "    predictions_top_one_percent = [1 if targets[i] in list(np.argpartition(-last_predictions[i], percent)[:percent]) else 0 for i in range(len(batch_inputs))]\n",
    "    predictions_top_one_percent = 100*np.count_nonzero(np.array(predictions_top_one_percent))/len(batch_inputs)\n",
    "\n",
    "    predictions_sorted = [1-len(np.where(last_predictions[i]>last_predictions[i][int(targets[i])])[0])/vocabulary_size for i in range(len(batch_inputs))]\n",
    "    MPR_sorted = 100*np.mean(np.array(predictions_sorted))\n",
    "    predictions_sorted_random = [1-(len(np.where(last_predictions[i]>last_predictions[i][int(np.random.randint(vocabulary_size-1))])[0])/vocabulary_size) for i in range(len(batch_inputs))]\n",
    "    return predictions_top_one, predictions_top_one_percent, MPR_sorted\n",
    "   \n",
    "\n",
    "def print_info_on_data(data):\n",
    "    Counterr = Counter([len(elem) for elem in data])\n",
    "    num_seq = len(data)\n",
    "    proportion_below = 20\n",
    "    num_seq_smaller_than = sum([Counterr[i] for i in range(1,proportion_below+1)])\n",
    "    print(\"Number of users = \"+ str(len(data)))\n",
    "    print('max length basket= ', max([len(elem) for elem in data]))\n",
    "    print('vocabulary size ', max([max(elem) for elem in data]))\n",
    "    print('proportion of baskets with size below ', str(proportion_below),' ', 100*num_seq_smaller_than/num_seq)\n",
    "    print(data[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual interesting code starts here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, type_of_data, dataset):\n",
    "        self.data, self.folder = load_data(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.type_of_data = type_of_data\n",
    "        shuffle(self.data)\n",
    "    \n",
    "        #Splitting the data\n",
    "        self.proportion_training, self.proportion_test = 0.8, 0.2\n",
    "        self.num_training_instances, self.num_test_instances = int(len(self.data)*self.proportion_training), int(len(self.data)*self.proportion_test)\n",
    "        self.test_data, self.training_data = self.data[:self.num_test_instances], self.data[self.num_test_instances:]\n",
    "        print(\"Length data, train and test \", len(self.data), len(self.training_data), len(self.test_data))\n",
    "        self.test_list_batches = get_test_list_batches(self, max_basket_size=10, number_of_baskets_per_size=1000)\n",
    "        self.test_data_size = min(2500, len(self.test_data))\n",
    "        self.test_baskets = [self.test_data[i] for i in sorted(random.sample(range(len(self.test_data)), self.test_data_size))]\n",
    "\n",
    "        _, self.number_of_products = counters_per_prod(self.data)\n",
    "        self.vocabulary_size = self.number_of_products if (type_of_data==\"real\") else 10000\n",
    "        self.embedding_size = 64\n",
    "        self.batch_size = 100\n",
    "        self.epoch = 10\n",
    "        self.neg_sampled = 50\n",
    "        self.seq_length= 25\n",
    "        self.use_pretrained_embeddings=False\n",
    "\n",
    "        self.popularity_distribution = np.array(get_popularity_dist(self.training_data, self.vocabulary_size))\n",
    "        print(\"Vocabulary size\", self.vocabulary_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gen_CNN_Model(object):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model_params = model\n",
    "        #Shared params\n",
    "        \n",
    "        #Number of products\n",
    "        self.vocabulary_size = model.vocabulary_size\n",
    "        \n",
    "        #Size of embedding vector for each product\n",
    "        self.embedding_size = model.embedding_size\n",
    "        \n",
    "        #Max basket size (sequence length)\n",
    "        self.seq_length = model.seq_length\n",
    "        \n",
    "        self.batch_size = model.batch_size\n",
    "        self.num_epochs = model.epoch\n",
    "        self.number_of_training_step = model.number_of_training_steps\n",
    "        self.embedding_matrix = model.embedding_matrix\n",
    "        self.use_pretrained_embeddings = model.use_pretrained_embeddings\n",
    "        self.training_data, self.test_data, self.test_list_batches, self.test_baskets = model.training_data, model.test_data, model.test_list_batches, model.test_baskets\n",
    "        \n",
    "        #CNN specific params\n",
    "        self.neg_sampled = 50\n",
    "        self.learning_rate = 1e-1\n",
    "        \n",
    "        \n",
    "        #Convolution paramsss !\n",
    "        self.number_of_convolutions = 2\n",
    "        self.num_filters = 100\n",
    "        self.filter_sizes = [2, 3, 5, 10]\n",
    "        self.max_pooling_window = 4\n",
    "     \n",
    "        self.second_filter_size = 3\n",
    "        self.num_second_filters = 100\n",
    "        self.second_max_pooling_window = 4\n",
    "\n",
    "        \n",
    "        self.printing_step = 100\n",
    "        self.X_train, self.Y_train, self.X_test, self.Y_test = list(), list(), list(), list()\n",
    "\n",
    "\n",
    "    def create_embedding_layer(self):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if (self.use_pretrained_embeddings):\n",
    "                print(\"Initializing with the previous embedding matrix\")\n",
    "                self.embeddings = Embedding(self.vocabulary_size, self.embedding_size, weights=[self.embedding_matrix], input_length=self.seq_length, trainable=True)\n",
    "            else:\n",
    "                print(\"Initializing with a random embedding matrix\")\n",
    "                self.embeddings = Embedding(self.vocabulary_size, self.embedding_size, embeddings_initializer='uniform', input_length=self.seq_length)\n",
    "            return self.embeddings\n",
    "\n",
    "    def convolve_4d_matrix(self, input_matrix, filter_shape, filter_size, num_filters, pooling_window_size):\n",
    "        \n",
    "        # tf.constant - Create the bias vector of size [num_filters] and value 0.1\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b-%s\" % filter_size,trainable=True)\n",
    "        \n",
    "        # Create the filter tensor with filter_shape and initialize it using truncated_normal\n",
    "        self.filter = tf.Variable(tf.truncated_normal(filter_shape,mean= 0.0, stddev=0.1), name=\"filter-%s\" % filter_size,trainable=True)\n",
    "                \n",
    "        # Computes a 2D convolution given 4D input and filter tensors.\n",
    "        # Given an input tensor of shape [batch, in_height, in_width, in_channels] and \n",
    "        #  a filter / kernel tensor of shape [filter_height, filter_width, in_channels=1, out_channels], \n",
    "        #  this op performs the following:\n",
    "        # - Input:\n",
    "        #   * InputMatrix - 4D Tensor [batch_size, seq_length, embedding_size, 1]\n",
    "        #   * Filter      - 4D Tensor [filter_size, embedding_size, 1, num_filters] \n",
    "        # - Flattens the filter to a 2-D matrix with shape: \n",
    "        #   [filter_height * filter_width * in_channels=1, output_channels]\n",
    "        # - Extracts image patches from the input tensor to form a virtual tensor of shape: \n",
    "        #   [batch, out_height, out_width, filter_height * filter_width * in_channels].\n",
    "        # - For each patch, right-multiplies the filter matrix and the image patch vector.\n",
    "        #\n",
    "        # padding = \"VALID\" means no padding so shrinks the input tensor\n",
    "        #\n",
    "        # OUTPUT DIMENSIONS: 4D [batch_size, new_length=seq_length-filter_size+1, 1, num_filters=100]\n",
    "        conv1 = tf.nn.conv2d(input_matrix, self.filter, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv1\")\n",
    "        self.conv1 = conv1 \n",
    "\n",
    "        # tf.nn.bias_add(conv1, bias) - ADDS BIAS (1D VECTOR) TO EACH CONVOLVED VCT IN CONV1 (100x)\n",
    "        # RELU = max(0, input)\n",
    "        h1 = tf.nn.relu(tf.nn.bias_add(conv1, bias), name=\"relu1\")\n",
    "\n",
    "        # tf.nn.max_pool\n",
    "        # h1 - example: [n-gram=2, embedding_size=100, 1, num_filters=100]\n",
    "        # ksize: The size of the pooling window for each dimension of the input tensor.\n",
    "        # e.g. [in the batch dim 1 per example, post_conv_filter_dim, width=1, 1 per filter]\n",
    "        # Compute max value per each filter\n",
    "        # OUTPUT DIMENSIONS: 4D [batch_size, 1, 1, num_filter=100]        \n",
    "        if (pooling_window_size>0):\n",
    "            pooled = tf.nn.max_pool(h1, ksize= [1, pooling_window_size, 1, 1], strides=[1,1,1,1], padding='VALID', name=\"pool1\")\n",
    "        else:\n",
    "            pooled = h1            \n",
    "        return tf.transpose(pooled, perm=[0,1,3,2])\n",
    "                \n",
    "        \n",
    "        \n",
    "    def creating_layer(self, input_tensor, dropout):\n",
    "        \n",
    "        embed = self.embeddings(input_tensor)\n",
    "        \n",
    "        #Reshapes the batch tensor from 3D to 4D since this it the input exp. by conv2D\n",
    "        self.embedded_chars_expanded = tf.expand_dims(embed, -1)\n",
    "        \n",
    "        pooled_outputs = []\n",
    "        \n",
    "        with tf.name_scope(\"conv-maxpool\"):\n",
    "            \n",
    "            for filter_size in self.filter_sizes:\n",
    "                \n",
    "                # Declare the filter shape [filter_height, filter_width, in_channels, out_channels]\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                pooled = self.convolve_4d_matrix(self.embedded_chars_expanded, filter_shape, filter_size, \\\n",
    "                    self.num_filters, pooling_window_size=self.max_pooling_window)\n",
    "                \n",
    "                #############################\n",
    "                ## Q1: 2nd conv goes here! ##\n",
    "                #############################\n",
    "                \n",
    "                pooled = tf.reduce_max(pooled, axis=1, keepdims=True)\n",
    "                \n",
    "                # Add the current pooled output\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Total filters: num_filters_per_type * num_filter_types\n",
    "        num_filters_total_per_cnn = self.num_second_filters #self.num_filters * len(self.filter_sizes)\n",
    "        \n",
    "        # TF.concat along the 3rd axis \n",
    "        # OUTPUT DIMENSIONS: 4D [batch_size, 1, 1, num_filters=100 * 4]\n",
    "        self.h_pool1 = tf.concat(pooled_outputs, 2)\n",
    "        \n",
    "        # TF.reshape\n",
    "        # OUTPUT DIMENSIONS: 2D [batch_size, num_filters=100 * 4]\n",
    "        self.h_drop1 = tf.reshape(self.h_pool1, [-1, self.num_filters * len(self.filter_sizes)]) if (self.number_of_convolutions==1) \\\n",
    "            else tf.reshape(self.h_pool1, [-1, self.num_second_filters * len(self.filter_sizes)])\n",
    "        \n",
    "        self.h_drop1 = tf.nn.dropout(self.h_drop1, dropout)\n",
    "        return self.h_drop1\n",
    "\n",
    "    \n",
    "    def compute_loss(self, output, label_tensor):\n",
    "        with tf.name_scope(\"output_layer\"):\n",
    "            self.nce_weights = tf.Variable(tf.truncated_normal([self.vocabulary_size, self.num_filters * len(self.filter_sizes)], stddev=1.0 / math.sqrt(self.embedding_size)))\n",
    "            self.nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "\n",
    "            #logits = tf.add((tf.matmul(output, tf.transpose(self.nce_weights))), self.nce_biases)\n",
    "            #self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels= label_tensor))\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "                weights=self.nce_weights, biases=self.nce_biases, labels=label_tensor,\n",
    "                inputs=output, num_sampled=self.neg_sampled, num_classes=self.vocabulary_size))      \n",
    "\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        return self.loss\n",
    "\n",
    "    \n",
    "    def create_graph(self):\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            self.train_words = tf.placeholder(tf.int32, shape=[None, None], name=\"train_inputs\")\n",
    "            self.label_words = tf.placeholder(tf.int32, shape=[None, None], name=\"train_labels\")\n",
    "            self.dropout = tf.placeholder(tf.float32, name=\"dropout\")\n",
    "        \n",
    "        self.create_embedding_layer()\n",
    "        \n",
    "        #Use TF.tile to repeat the elements until target seq. length\n",
    "        self.context_words = tf.tile(self.train_words, tf.constant([1, self.seq_length]))[:,:self.seq_length]\n",
    "        \n",
    "        self.output = self.creating_layer(self.context_words, self.dropout)\n",
    "        \n",
    "        self.loss = self.compute_loss(self.output, tf.reshape(self.label_words[:,-1], (-1,1)))\n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            optimizer_plot = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "            grads_and_vars = optimizer_plot.compute_gradients(self.loss)\n",
    "            self.train_op = optimizer_plot.apply_gradients(grads_and_vars, global_step=self.global_step)\n",
    "        \n",
    "        self.get_predictions(self.output)\n",
    "        self.get_scores(self.output)\n",
    "        \n",
    "    def get_predictions(self, output):\n",
    "        self.last_predictions = tf.nn.softmax(tf.matmul(output,tf.transpose(self.nce_weights))+self.nce_biases)\n",
    "        return self.last_predictions\n",
    "    \n",
    "    def get_scores(self, output):\n",
    "        self.before_softmax = tf.matmul(output,tf.transpose(self.nce_weights))+self.nce_biases\n",
    "        return self.before_softmax\n",
    "\n",
    "    def train_model_with_tensorflow(self):\n",
    "        self.create_graph()\n",
    "        self._sess = tf.Session()\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        total_loss, data_idx, step = 0, 0, 0\n",
    "        steps_per_epoch = int(len(self.X_train)/self.batch_size)\n",
    "        MPRs, precision1s, precision1ps = list(), list(), list()\n",
    "        train_words, label_words = create_batch(self)\n",
    "        _, loss, h_drop1 = self._sess.run([self.optimizer, self.loss, self.h_drop1], feed_dict={self.train_words:train_words, self.label_words:label_words, self.dropout:1})\n",
    "        print(h_drop1.shape)\n",
    "        \n",
    "        while (step < self.number_of_training_step):\n",
    "            \n",
    "            train_words, label_words = create_batch(self)\n",
    "            _, loss = self._sess.run([self.optimizer, self.loss], feed_dict={self.train_words:train_words, self.label_words:label_words, self.dropout:1})\n",
    "            \n",
    "            total_loss += loss\n",
    "            step += 1\n",
    "\n",
    "            if (step % self.printing_step == 0) or (step==1):\n",
    "                print(\"Step \"+ str(step) +\" and loss \" + str(total_loss/self.printing_step))\n",
    "                total_loss = 0\n",
    "\n",
    "                input_w, target_w, last_predictions = list(), list(), list()\n",
    "                for elem in self.test_baskets:\n",
    "                    train_words, label_words = np.reshape(np.array(elem[:-1]), (1,-1)), np.reshape(np.array(elem[1:]), (-1, 1))\n",
    "                    input_w.append(train_words)\n",
    "                    target_w.append(label_words[-1])\n",
    "                    \n",
    "                    last_predictions.append(np.reshape(self._sess.run([self.before_softmax], feed_dict={self.train_words:train_words, self.label_words:label_words, self.dropout:1})[0], [-1]))\n",
    "                \n",
    "                precision1, precision1p, MPR = print_results_predictions(np.array(last_predictions), input_w, np.array(target_w).flatten(), self.vocabulary_size)\n",
    "                MPRs.append(MPR)\n",
    "                precision1s.append(precision1)\n",
    "                precision1ps.append(precision1p)\n",
    "                print(\"MPR \"+ str(round(MPR, 3))+ \" Prec@1 \"+ str(precision1)+ \" Prec@1pct \"+ str(precision1p))\n",
    "                print(\"\")\n",
    "\n",
    "        data = np.array([MPRs, precision1s, precision1ps])\n",
    "        np.save(\"textCNN_results\", data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Model):\n",
    "    def __init__(self, type_of_data, dataset, number_of_training_steps):\n",
    "        Model.__init__(self, type_of_data, dataset)\n",
    "        embedding_matrix = np.load(\"embedding_matrix.npy\") if (os.path.isfile(\"embedding_matrix.npy\")) else np.zeros((1,1))\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.number_of_training_steps = number_of_training_steps\n",
    "        cnn = Gen_CNN_Model(self)\n",
    "        cnn.train_model_with_tensorflow()\n",
    "\n",
    "\n",
    "cnn = CNN(type_of_data=\"real\", dataset=\"Amazon\", number_of_training_steps=20000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow\n",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
