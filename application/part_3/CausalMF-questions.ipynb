{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Predict User Item response under uniform exposure while learning from biased training data\n",
    "\n",
    "ÂŒMany current applications use recommendations in order to modify the natural user behavior, such as to increase the number of sales or the time spent on a website. This results in a gap between the final recommendation objective and the classical setup where recommendation candidates are evaluated by their coherence with past user behavior, by predicting either the missing entries in the user-item matrix, or the most likely next event. To bridge this gap, we optimize a recommendation policy for the task of increasing the desired outcome versus the organic user behavior. We show this is equivalent to learning to predict recommendation outcomes under a fully random recommendation policy. To this end, we propose a new domain adaptation algorithm that learns from logged data containing outcomes from a biased recommendation policy and predicts recommendation outcomes according to random exposure. We compare our method against state-of-the-art factorization methods and new approaches of causal recommendation and show significant improvements.\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "**MovieLens 100k dataset** was collected by the GroupLens Research Project at the University of Minnesota.\n",
    " \n",
    "This data set consists of:\n",
    "\t* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "\t* Each user has rated at least 20 movies. \n",
    "\n",
    "The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998.\n",
    "\n",
    "\n",
    "\n",
    "# Solution:\n",
    "\n",
    "**Causal Matrix Factorization** - for more details see: https://arxiv.org/abs/1706.07639\n",
    "\n",
    "![TextCNN](./images/causalMF.png)\n",
    "\n",
    "\n",
    "# Metrics:\n",
    "\n",
    "### * MSE - Mean Squared Error\n",
    "### * NLL - Negative Log Likelihood\n",
    "### * AUC - Area Under the Curve\n",
    "\n",
    "\n",
    "-----------------------------\n",
    "-----------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Questions:\n",
    "\n",
    "\n",
    "### Q1: Add the definition for create_counterfactual_regularizer() method\n",
    "### Q2: Compare the results of using variable values for cf_pen hyperparameter (0 vs. bigger)\n",
    "### Q3: Compare different types of optimizers\n",
    "### Q4: Push the performance as high as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "flags = tf.app.flags \n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "flags.DEFINE_string('data_set', 'user_prod_dict.skew.', 'Dataset string.')  # Reg Skew\n",
    "flags.DEFINE_string('adapt_stat', 'adapt_2i', 'Adapt String.')  # Adaptation strategy\n",
    "flags.DEFINE_string('model_name', 'cp2v', 'Name of the model for saving.')\n",
    "flags.DEFINE_float('learning_rate', 1.0, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('num_epochs', 1, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('num_steps', 100, 'Number of steps after which to test.')\n",
    "flags.DEFINE_integer('embedding_size', 100, 'Size of each embedding vector.')\n",
    "flags.DEFINE_integer('batch_size', 512, 'How big is a batch of training.')\n",
    "flags.DEFINE_float('cf_pen', 10.0, 'Counterfactual regularizer hyperparam.')\n",
    "flags.DEFINE_float('l2_pen', 0.0, 'L2 regularizer hyperparam.')\n",
    "flags.DEFINE_string('cf_loss', 'l1', 'Use L1 or L2 for the loss .')\n",
    "FLAGS = tf.app.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_DATA_PATH = \"/Users/f.vasile/MyFolders/MyProjects/1.MyPapers/2018_Q2_DS3_Course/code/cp2v/src/Data/\"\n",
    "_DATA_PATH = \"./data/\"\n",
    "\n",
    "train_data_set_location = _DATA_PATH + FLAGS.data_set +  \"train.\" + FLAGS.adapt_stat + \".csv\" # Location of train dataset\n",
    "test_data_set_location = _DATA_PATH + FLAGS.data_set +  \"test.\" + FLAGS.adapt_stat + \".csv\" # Location of the test dataset\n",
    "validation_test_set_location = _DATA_PATH + FLAGS.data_set +  \"valid_test.\" + FLAGS.adapt_stat + \".csv\" # Location of the validation dataset\n",
    "validation_train_set_location = _DATA_PATH + FLAGS.data_set +  \"valid_train.\" + FLAGS.adapt_stat + \".csv\" #Location of the validation dataset\n",
    "model_name = FLAGS.model_name + \".ckpt\"\n",
    "\n",
    "print(train_data_set_location)\n",
    "\n",
    "\n",
    "def calculate_vocab_size(file_location):\n",
    "    \"\"\"Calculate the total number of unique elements in the dataset\"\"\"\n",
    "\n",
    "    with open(file_location, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        useridtemp = []\n",
    "        productid = []\n",
    "        for row in reader:\n",
    "            useridtemp.append(row[0])\n",
    "            productid.append(row[1])\n",
    "\n",
    "    userid_size = len(set(useridtemp))\n",
    "    productid_size = len(set(productid))\n",
    "\n",
    "    return userid_size, productid_size\n",
    "\n",
    "\n",
    "userid_size, productid_size = calculate_vocab_size(train_data_set_location) # Calculate the total number of unique elements in the dataset\n",
    "\n",
    "print(str(userid_size))\n",
    "print(str(productid_size))\n",
    "\n",
    "plot_gradients = False # Plot the gradients\n",
    "cost_val = []\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_dataset(dataset_location, batch_size, num_epochs):\n",
    "    \"\"\"Load the training data using TF Dataset API\"\"\"\n",
    "\n",
    "    with tf.name_scope('train_dataset_loading'):\n",
    "\n",
    "        record_defaults = [[1], [1], [0.]] # Sets the type of the resulting tensors and default values\n",
    "        # Dataset is in the format - UserID ProductID Rating\n",
    "        dataset = tf.data.TextLineDataset(dataset_location).map(lambda line: tf.decode_csv(line, record_defaults=record_defaults))\n",
    "        dataset = dataset.shuffle(buffer_size=10000)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        user_batch, product_batch, label_batch = iterator.get_next()\n",
    "        label_batch = tf.expand_dims(label_batch, 1)\n",
    "\n",
    "    return user_batch, product_batch, label_batch\n",
    "\n",
    "\n",
    "def load_test_dataset(dataset_location):\n",
    "    \"\"\"Load the test and validation datasets\"\"\"\n",
    "\n",
    "    user_list = []\n",
    "    product_list = []\n",
    "    labels = []\n",
    "\n",
    "    with open(dataset_location, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            user_list.append(row[0])\n",
    "            product_list.append(row[1])\n",
    "            labels.append(row[2])\n",
    "\n",
    "    labels = np.reshape(labels, [-1, 1])\n",
    "    cr = compute_empirical_cr(labels)\n",
    "\n",
    "    return user_list, product_list, labels, cr\n",
    "\n",
    "\n",
    "def compute_2i_regularization_id(prods, num_products):\n",
    "    \"\"\"Compute the ID for the regularization for the 2i approach\"\"\"\n",
    "\n",
    "    reg_ids = []\n",
    "    # Loop through batch and compute if the product ID is greater than the number of products\n",
    "    for x in np.nditer(prods):\n",
    "        if x >= num_products:\n",
    "            reg_ids.append(x)\n",
    "        elif x < num_products:\n",
    "            reg_ids.append(x + num_products) # Add number of products to create the 2i representation \n",
    "\n",
    "    return np.asarray(reg_ids)\n",
    "\n",
    "\n",
    "def generate_bootstrap_batch(seed, data_set_size):\n",
    "    \"\"\"Generate the IDs for the bootstap\"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    ids = [random.randint(0, data_set_size-1) for j in range(int(data_set_size*0.8))]\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def compute_empirical_cr(labels):\n",
    "    \"\"\"Compute the cr from the empirical data\"\"\"\n",
    "\n",
    "    labels = labels.astype(np.float)\n",
    "    clicks = np.count_nonzero(labels)\n",
    "    views = len(np.where(labels==0)[0])\n",
    "    cr = float(clicks)/float(views)\n",
    "\n",
    "    return cr\n",
    "\n",
    "\n",
    "def create_average_predictor_tensors(label_list_placeholder, logits_placeholder):\n",
    "    \"\"\"Create the tensors required to run the averate predictor for the bootstraps\"\"\"\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        \n",
    "        with tf.variable_scope('ap_logits'):\n",
    "            ap_logits = tf.reshape(logits_placeholder, [tf.shape(label_list_placeholder)[0], 1])\n",
    "\n",
    "        with tf.name_scope('ap_losses'):\n",
    "            \n",
    "            ap_mse_loss = tf.losses.mean_squared_error(labels=label_list_placeholder, predictions=ap_logits)\n",
    "            ap_log_loss =  tf.losses.log_loss(labels=label_list_placeholder, predictions=ap_logits)\n",
    "\n",
    "        with tf.name_scope('ap_metrics'):\n",
    "            # Add performance metrics to the tensorflow graph\n",
    "            ap_correct_predictions = tf.equal(tf.round(ap_logits), label_list_placeholder)\n",
    "            ap_accuracy = tf.reduce_mean(tf.cast(ap_correct_predictions, tf.float32))\n",
    "\n",
    "    return ap_mse_loss, ap_log_loss\n",
    "\n",
    "def compute_bootstraps_2i(sess, model, test_user_batch, test_product_batch, test_label_batch, test_logits, running_vars_initializer, ap_mse_loss, ap_log_loss):\n",
    "    \"\"\"Compute the bootstraps for the 2i model\"\"\"\n",
    "    \n",
    "    data_set_size = len(test_user_batch)\n",
    "    mse = []\n",
    "    llh = []\n",
    "    ap_mse = []\n",
    "    ap_llh = []\n",
    "    auc_list = []\n",
    "    mse_diff = []\n",
    "    llh_diff = []\n",
    "\n",
    "    # Compute the bootstrap values for the test split - this compute the empirical CR as well for comparision\n",
    "    for i in range(30):\n",
    "\n",
    "        ids = generate_bootstrap_batch(i*2, data_set_size)\n",
    "        test_user_batch = np.asarray(test_user_batch)\n",
    "        test_product_batch = np.asarray(test_product_batch)\n",
    "        test_label_batch = np.asarray(test_label_batch)\n",
    "\n",
    "        # Reset the running variables used for the AUC\n",
    "        sess.run(running_vars_initializer)\n",
    "\n",
    "        # Construct the feed-dict for the model and the average predictor\n",
    "        feed_dict = {model.user_list_placeholder : test_user_batch[ids], model.product_list_placeholder: test_product_batch[ids], model.label_list_placeholder: test_label_batch[ids], model.logits_placeholder: test_logits[ids], model.reg_list_placeholder: test_product_batch[ids]}\n",
    "\n",
    "        # Run the model test step updating the AUC object\n",
    "        _, loss_val, mse_loss_val, log_loss_val = sess.run([model.auc_update_op, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict)\n",
    "        auc_score = sess.run(model.auc, feed_dict=feed_dict)\n",
    "\n",
    "        # Run the Average Predictor graph\n",
    "        ap_mse_val, ap_log_val = sess.run([ap_mse_loss, ap_log_loss], feed_dict=feed_dict)\n",
    "\n",
    "        mse.append(mse_loss_val)\n",
    "        llh.append(log_loss_val)\n",
    "        ap_mse.append(ap_mse_val)\n",
    "        ap_llh.append(ap_log_val)\n",
    "        auc_list.append(auc_score)\n",
    "\n",
    "    for i in range(30):\n",
    "        mse_diff.append((ap_mse[i]-mse[i]) / ap_mse[i])\n",
    "        llh_diff.append((ap_llh[i]-llh[i]) / ap_llh[i])\n",
    "\n",
    "    print(\"MSE Mean Score On The Bootstrap = \", np.mean(mse))\n",
    "    print(\"MSE Mean Lift Over Average Predictor (%) = \", np.round(np.mean(mse_diff)*100, decimals=2))\n",
    "    print(\"MSE STD (%) =\" , np.round(np.std(mse_diff)*100, decimals=2))\n",
    "\n",
    "    print(\"LLH Mean Over Average Predictor (%) =\", np.round(np.mean(llh_diff)*100, decimals=2))\n",
    "    print(\"LLH STD (%) = \", np.round(np.std(llh_diff)*100, decimals=2))\n",
    "\n",
    "    print(\"Mean AUC Score On The Bootstrap = \",  np.round(np.mean(auc_list), decimals=4), \"+/-\", np.round(np.std(auc_list), decimals=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Supervised Prod2vec \n",
    "\n",
    "- Class to define MF of the implicit feedback matrix (1/0/unk) of Users x Products\n",
    "\n",
    "- When called it creates the TF graph for the associated NN:\n",
    "\n",
    "Step1: self.create_placeholders() => Creates the input placeholders\n",
    "\n",
    "Step2: self.build_graph() => Creates the 3 layers: \n",
    "    - the user embedding layer\n",
    "    - the product embedding layer \n",
    "    - the output prediction layer\n",
    "\n",
    "Step3: self.create_losses() => Defines the loss function for prediction\n",
    "\n",
    "Step4: self.add_optimizer() => Defines the optimizer\n",
    "\n",
    "Step5: self.add_performance_metrics() => Defines the logging performance metrics ???\n",
    "\n",
    "Step6: self.add_summaries() => Defines the final performance stats\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SupervisedProd2vec():\n",
    "    def __init__(self, userid_size, productid_size, embedding_size, l2_pen, learning_rate):\n",
    "\n",
    "        self.userid_size = userid_size\n",
    "        self.productid_size = productid_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.l2_pen = l2_pen\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Build the graph\n",
    "        self.create_placeholders()\n",
    "        self.build_graph()\n",
    "        self.create_losses()\n",
    "        self.add_optimizer()\n",
    "        self.add_performance_metrics()\n",
    "        self.add_summaries()\n",
    "        \n",
    "    def create_placeholders(self):\n",
    "        \"\"\"Create the placeholders to be used \"\"\"\n",
    "        \n",
    "        self.user_list_placeholder = tf.placeholder(tf.int32, [None], name=\"user_list_placeholder\")\n",
    "        self.product_list_placeholder = tf.placeholder(tf.int32, [None], name=\"product_list_placeholder\")\n",
    "        self.label_list_placeholder = tf.placeholder(tf.float32, [None, 1], name=\"label_list_placeholder\")\n",
    "\n",
    "        # logits placeholder used to store the test CR for the bootstrapping process\n",
    "        self.logits_placeholder = tf.placeholder(tf.float32, [None], name=\"logits_placeholder\")\n",
    "\n",
    "        \n",
    "    def build_graph(self):\n",
    "        \"\"\"Build the main tensorflow graph with embedding layers\"\"\"\n",
    "\n",
    "        with tf.name_scope('embedding_layer'):\n",
    "\n",
    "            # User matrix and current batch\n",
    "            self.user_embeddings = tf.get_variable(\"user_embeddings\", shape=[self.userid_size, self.embedding_size], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "            self.user_embed = tf.nn.embedding_lookup(self.user_embeddings, self.user_list_placeholder) # Lookup the Users for the given batch\n",
    "            self.user_b = tf.Variable(tf.zeros([self.userid_size]), name='user_b', trainable=True)\n",
    "            self.user_bias_embed = tf.nn.embedding_lookup(self.user_b, self.user_list_placeholder)\n",
    "\n",
    "            # Product embedding\n",
    "            self.product_embeddings = tf.get_variable(\"product_embeddings\", shape=[self.productid_size, self.embedding_size], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "            self.product_embed = tf.nn.embedding_lookup(self.product_embeddings, self.product_list_placeholder) # Lookup the embeddings2 for the given batch\n",
    "            self.prod_b = tf.Variable(tf.zeros([self.productid_size]), name='prod_b', trainable=True)\n",
    "            self.prod_bias_embed = tf.nn.embedding_lookup(self.prod_b, self.product_list_placeholder)\n",
    "\n",
    "        with tf.variable_scope('logits'):\n",
    "\n",
    "            self.b = tf.get_variable('b', [1], initializer=tf.constant_initializer(0.0, dtype=tf.float32), trainable=True)\n",
    "            self.alpha = tf.get_variable('alpha', [], initializer=tf.constant_initializer(0.00000001, dtype=tf.float32), trainable=True)\n",
    "            \n",
    "            #alpha * (<user_i, prod_j> \n",
    "            self.emb_logits = self.alpha * tf.reshape(tf.reduce_sum(tf.multiply(self.user_embed, self.product_embed), 1), [tf.shape(self.user_list_placeholder)[0], 1])\n",
    "            \n",
    "            #prod_bias + user_bias + global_bias\n",
    "            self.logits = tf.reshape(tf.add(self.prod_bias_embed, self.user_bias_embed), [tf.shape(self.user_list_placeholder)[0], 1]) + self.b\n",
    "            \n",
    "            self.logits = self.emb_logits + self.logits\n",
    "\n",
    "            self.prediction = tf.sigmoid(self.logits, name='sigmoid_prediction')\n",
    "\n",
    "            \n",
    "    def create_losses(self):\n",
    "        \"\"\"Create the losses\"\"\"\n",
    "\n",
    "        with tf.name_scope('losses'):\n",
    "            #Sigmoid loss between the logits and labels\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.label_list_placeholder))\n",
    "            \n",
    "            #Adding the regularizer term on user vct and prod vct\n",
    "            self.loss = self.loss + self.l2_pen * tf.nn.l2_loss(self.user_embeddings) + self.l2_pen * tf.nn.l2_loss(self.product_embeddings) + self.l2_pen * tf.nn.l2_loss(self.prod_b) + self.l2_pen * tf.nn.l2_loss(self.user_b)\n",
    "\n",
    "            #Compute MSE loss\n",
    "            self.mse_loss = tf.losses.mean_squared_error(labels=self.label_list_placeholder, predictions=tf.sigmoid(self.logits))\n",
    "            \n",
    "            #Compute Log loss\n",
    "            self.log_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.label_list_placeholder))\n",
    "            \n",
    "            \n",
    "    def add_optimizer(self):\n",
    "        \"\"\"Add the required optimiser to the graph\"\"\"\n",
    "\n",
    "        with tf.name_scope('optimizer'):\n",
    "            # Global step variable to keep track of the number of training steps\n",
    "            self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')   \n",
    "            self.apply_grads = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "                \n",
    "    def add_performance_metrics(self):\n",
    "        \"\"\"Add the required performance metrics to the graph\"\"\"\n",
    "        \n",
    "        with tf.name_scope('performance_metrics'):\n",
    "            # Add performance metrics to the tensorflow graph\n",
    "            correct_predictions = tf.equal(tf.round(self.prediction), self.label_list_placeholder)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=\"accuracy\")\n",
    "            self.auc, self.auc_update_op = tf.metrics.auc(labels=self.label_list_placeholder, predictions=self.prediction, num_thresholds=1000, name=\"auc_metric\")\n",
    "\n",
    "            \n",
    "    def add_summaries(self):\n",
    "        \"\"\"Add the required summaries to the graph\"\"\"\n",
    "\n",
    "        with tf.name_scope('summaries'):\n",
    "            # Add loss to the summaries\n",
    "            tf.summary.scalar('total_loss', self.loss)\n",
    "            tf.summary.histogram('histogram_total_loss', self.loss)\n",
    "\n",
    "            # Add weights to the summaries\n",
    "            tf.summary.histogram('user_embedding_weights', self.user_embeddings)\n",
    "            tf.summary.histogram('product_embedding_weights', self.product_embeddings)\n",
    "            tf.summary.histogram('logits', self.logits)\n",
    "            tf.summary.histogram('prod_b', self.prod_b)\n",
    "            tf.summary.histogram('user_b', self.user_b)\n",
    "            tf.summary.histogram('global_bias', self.b)\n",
    "\n",
    "            tf.summary.scalar('alpha', self.alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CausalProd2Vec2i - inherits from SupervisedProd2vec\n",
    "\n",
    "- Class to define the causal version of MF of the implicit feedback matrix (1/0/unk) of Users x Products\n",
    "\n",
    "- When called it creates the TF graph for the associated NN:\n",
    "\n",
    "**Step1: Changed: +regularizer placeholder** self.create_placeholders() => Creates the input placeholders \n",
    "\n",
    "**Step2:** self.build_graph() => Creates the 3 layers: \n",
    "    - the user embedding layer\n",
    "    - the product embedding layer \n",
    "    - the output prediction layer\n",
    "\n",
    "**New:**\n",
    "\n",
    "        self.create_control_embeddings()\n",
    "        self.create_counter_factual_loss()\n",
    "\n",
    "\n",
    "**Step3: Changed: +add regularizer between embeddings** self.create_losses() => Defines the loss function for prediction\n",
    "\n",
    "**Step4:** self.add_optimizer() => Defines the optimizer\n",
    "\n",
    "**Step5:** self.add_performance_metrics() => Defines the logging performance metrics ???\n",
    "\n",
    "**Step6:** self.add_summaries() => Defines the final performance stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CausalProd2Vec2i(SupervisedProd2vec):\n",
    "    def __init__(self, userid_size, productid_size, embedding_size, l2_pen, learning_rate, cf_pen, cf='l1'):\n",
    "\n",
    "        self.userid_size = userid_size\n",
    "        self.productid_size = productid_size * 2 # Doubled to accommodate the treatment embeddings \n",
    "        self.embedding_size = embedding_size\n",
    "        self.l2_pen = l2_pen\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cf_pen = cf_pen\n",
    "        self.cf = cf\n",
    "\n",
    "        # Build the graph\n",
    "        self.create_placeholders()\n",
    "        self.build_graph()\n",
    "        self.create_control_embeddings()\n",
    "        #self.create_counterfactual_regularizer()\n",
    "        self.create_losses()\n",
    "        self.add_optimizer()\n",
    "        self.add_performance_metrics()\n",
    "        self.add_summaries()\n",
    "        \n",
    "    def create_placeholders(self):\n",
    "        \"\"\"Create the placeholders to be used \"\"\"\n",
    "        \n",
    "        self.user_list_placeholder = tf.placeholder(tf.int32, [None], name=\"user_list_placeholder\")\n",
    "        self.product_list_placeholder = tf.placeholder(tf.int32, [None], name=\"product_list_placeholder\")\n",
    "        self.label_list_placeholder = tf.placeholder(tf.float32, [None, 1], name=\"label_list_placeholder\")\n",
    "        self.reg_list_placeholder = tf.placeholder(tf.int32, [None], name=\"reg_list_placeholder\")\n",
    "\n",
    "        # logits placeholder used to store the test CR for the bootstrapping process\n",
    "        self.logits_placeholder = tf.placeholder(tf.float32, [None], name=\"logits_placeholder\")\n",
    "\n",
    "        \n",
    "    def create_control_embeddings(self):\n",
    "        \"\"\"Create the control embeddings\"\"\"\n",
    "\n",
    "        with tf.name_scope('control_embedding'):\n",
    "            # Get the control embedding at id 0\n",
    "            self.control_embed = tf.stop_gradient(tf.nn.embedding_lookup(self.product_embeddings, self.reg_list_placeholder))\n",
    "  \n",
    "\n",
    "    #################################\n",
    "    ##  SOLUTION TO Q1 GOES HERE!  ##\n",
    "    #################################\n",
    "    #def create_counterfactual_regularizer(self):\n",
    "    \n",
    "    # self.cf_reg\n",
    "    \n",
    "                \n",
    "            \n",
    "    def create_losses(self):\n",
    "        \"\"\"Create the losses\"\"\"\n",
    "\n",
    "        with tf.name_scope('losses'):\n",
    "            #Sigmoid loss between the logits and labels\n",
    "            self.log_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.label_list_placeholder))\n",
    "            \n",
    "            #Adding the regularizer term on user vct and prod vct and their bias terms\n",
    "            reg_term = self.l2_pen * ( tf.nn.l2_loss(self.user_embeddings) + tf.nn.l2_loss(self.product_embeddings) )\n",
    "            reg_term_biases = self.l2_pen * ( tf.nn.l2_loss(self.prod_b) + tf.nn.l2_loss(self.user_b) )\n",
    "            self.loss = self.log_loss + reg_term + reg_term_biases\n",
    "            \n",
    "            #Adding the counterfactual regualizer term\n",
    "            # Q1: Write the method that computes the counterfactual regularizer\n",
    "            #self.create_counterfactual_regularizer()\n",
    "            #self.loss = self.loss + (self.cf_pen * self.cf_reg)\n",
    "\n",
    "            #Compute addtionally the MSE loss\n",
    "            self.mse_loss = tf.losses.mean_squared_error(labels=self.label_list_placeholder, predictions=tf.sigmoid(self.logits))\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the TF Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create graph object\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Load the required graph\n",
    "\n",
    "        ### Number of products and users\n",
    "        productid_size = 1683\n",
    "        userid_size = 944\n",
    "\n",
    "        model = CausalProd2Vec2i(userid_size, productid_size+1, FLAGS.embedding_size, FLAGS.l2_pen, FLAGS.learning_rate, FLAGS.cf_pen, cf=FLAGS.cf_loss)\n",
    "\n",
    "        ap_mse_loss, ap_log_loss = create_average_predictor_tensors(model.label_list_placeholder, model.logits_placeholder)\n",
    "        \n",
    "        # Define initializer to initialize/reset running variables\n",
    "        running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"performance_metrics/auc_metric\")\n",
    "        running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "        # Get train data batch from queue\n",
    "        next_batch = load_train_dataset(train_data_set_location, FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        test_user_batch, test_product_batch, test_label_batch, test_cr = load_test_dataset(test_data_set_location)\n",
    "        val_test_user_batch, val_test_product_batch, val_test_label_batch, val_cr = load_test_dataset(validation_test_set_location)\n",
    "        val_train_user_batch, val_train_product_batch, val_train_label_batch, val_cr = load_test_dataset(validation_train_set_location)\n",
    "\n",
    "        # create the empirical CR test logits \n",
    "        test_logits = np.empty(len(test_label_batch))\n",
    "        test_logits.fill(test_cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Session: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the Session\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n",
    "\n",
    "    # initialise all the TF variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Setup tensorboard: tensorboard --logdir=/tmp/tensorboard\n",
    "    time_tb = str(time.ctime(int(time.time())))\n",
    "    train_writer = tf.summary.FileWriter('/tmp/tensorboard' + '/train' + time_tb, sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('/tmp/tensorboard' + '/test' + time_tb, sess.graph)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Embeddings viz (Possible to add labels for embeddings later)\n",
    "    saver = tf.train.Saver()\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = model.product_embeddings.name\n",
    "    projector.visualize_embeddings(train_writer, config)\n",
    "\n",
    "    # Variables used in the training loop\n",
    "    t = time.time()\n",
    "    step = 0\n",
    "    average_loss = 0\n",
    "    average_mse_loss = 0\n",
    "    average_log_loss = 0\n",
    "\n",
    "    # Start the training loop---------------------------------------------------------------------------------------------\n",
    "    print(\"Starting Training On Causal Prod2Vec\")\n",
    "    print(FLAGS.cf_loss)\n",
    "    print(\"Num Epochs = \", FLAGS.num_epochs)\n",
    "    print(\"Learning Rate = \", FLAGS.learning_rate)\n",
    "    print(\"L2 Reg = \", FLAGS.l2_pen)\n",
    "    print(\"CF Reg = \", FLAGS.cf_pen)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Run the TRAIN for this step batch ---------------------------------------------------------------------\n",
    "            # Construct the feed_dict\n",
    "            user_batch, product_batch, label_batch = sess.run(next_batch)\n",
    "            # Treatment is the small set of samples from St, Control is the larger set of samples from Sc\n",
    "            reg_ids = compute_2i_regularization_id(product_batch, productid_size) # Compute the product ID's for regularization\n",
    "            feed_dict = {model.user_list_placeholder : user_batch, model.product_list_placeholder: product_batch, model.reg_list_placeholder: reg_ids, model.label_list_placeholder: label_batch}\n",
    "            \n",
    "            # Run the graph\n",
    "            _, sum_str, loss_val, mse_loss_val, log_loss_val = sess.run([model.apply_grads, merged, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict)\n",
    "\n",
    "            step +=1\n",
    "            average_loss += loss_val\n",
    "            average_mse_loss += mse_loss_val\n",
    "            average_log_loss += log_loss_val\n",
    "\n",
    "            # Every num_steps print average loss\n",
    "            if step % FLAGS.num_steps == 0:\n",
    "                if step > FLAGS.num_steps:\n",
    "                    # The average loss is an estimate of the loss over the last set batches.\n",
    "                    average_loss /= FLAGS.num_steps\n",
    "                    average_mse_loss /= FLAGS.num_steps\n",
    "                    average_log_loss /= FLAGS.num_steps\n",
    "                print(\"Average Training Loss on S_c (FULL, MSE, NLL) at step \", step, \": \", average_loss, \": \", average_mse_loss, \": \", average_log_loss, \"Time taken (S) = \" + str(round(time.time() - t, 1)))\n",
    "\n",
    "                average_loss = 0\n",
    "                t = time.time() # reset the time\n",
    "                train_writer.add_summary(sum_str, step) # Write the summary\n",
    "\n",
    "                # Run the VALIDATION for this step batch ---------------------------------------------------------------------\n",
    "                val_train_product_batch = np.asarray(val_train_product_batch, dtype=np.float32)\n",
    "                val_test_product_batch = np.asarray(val_test_product_batch, dtype=np.float32)\n",
    "                vaL_train_reg_ids = compute_2i_regularization_id(val_train_product_batch, productid_size) # Compute the product ID's for regularization\n",
    "                vaL_test_reg_ids = compute_2i_regularization_id(val_test_product_batch, productid_size) # Compute the product ID's for regularization\n",
    "                feed_dict_test = {model.user_list_placeholder : val_test_user_batch, model.product_list_placeholder: val_test_product_batch, model.reg_list_placeholder: vaL_test_reg_ids,  model.label_list_placeholder: val_test_label_batch}\n",
    "                feed_dict_train = {model.user_list_placeholder : val_train_user_batch, model.product_list_placeholder: val_train_product_batch, model.reg_list_placeholder: vaL_train_reg_ids, model.label_list_placeholder: val_train_label_batch}\n",
    "     \n",
    "                sum_str, loss_val, mse_loss_val, log_loss_val = sess.run([merged, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict_train)\n",
    "                print(\"Validation loss on S_c (FULL, MSE, NLL) at step \", step, \": \", loss_val, \": \", mse_loss_val, \": \", log_loss_val)\n",
    "            \n",
    "                sum_str, loss_val, mse_loss_val, log_loss_val = sess.run([merged, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict_test)\n",
    "                cost_val.append(loss_val)\n",
    "                print(\"Validation loss on S_t(FULL, MSE, NLL) at step \", step, \": \", loss_val, \": \", mse_loss_val, \": \", log_loss_val)\n",
    "                     \n",
    "                print(\"####################################################################################################################\")   \n",
    "\n",
    "                test_writer.add_summary(sum_str, step) # Write the summary\n",
    "                    \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"Reached the number of epochs\")\n",
    "\n",
    "    finally:\n",
    "        saver.save(sess, os.path.join('/tmp/tensorboard', model_name), model.global_step) # Save model\n",
    "\n",
    "    train_writer.close()\n",
    "    print(\"Training Complete\")\n",
    "\n",
    "    # Run the bootstrap for this model ---------------------------------------------------------------------------------------------------------------\n",
    "    print(\"Begin Bootstrap process...\")\n",
    "    print(\"Running BootStrap On The Control Representations\")\n",
    "    compute_bootstraps_2i(sess, model, test_user_batch, test_product_batch, test_label_batch, test_logits, running_vars_initializer, ap_mse_loss, ap_log_loss)\n",
    "\n",
    "    print(\"Running BootStrap On The Treatment Representations\")\n",
    "    test_product_batch = [int(x) + productid_size for x in test_product_batch]\n",
    "    compute_bootstraps_2i(sess, model, test_user_batch, test_product_batch, test_label_batch, test_logits, running_vars_initializer, ap_mse_loss, ap_log_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow\n",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
